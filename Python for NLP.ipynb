{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk import word_tokenize, pos_tag\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "import sklearn.feature_extraction.text\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./images/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from analytics vidhya'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"  \n",
    "\n",
    "_remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaing the root of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"multiplying\" \n",
    "lem.lemmatize(word, \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strripping off the suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by Shivam Bansal'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text to Features (Feature Engineering on text data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Tree\n",
    "![title](./images/trees.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Extraction (Entities as features)\n",
    "![title](./images/entity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.060*\"driving\" + 0.060*\"cause\" + 0.060*\"pressure.\" + 0.060*\"blood\" + 0.060*\"increased\" + 0.060*\"stress\" + 0.060*\"suggest\" + 0.060*\"Doctors\" + 0.060*\"and\" + 0.060*\"that\"'), (1, '0.053*\"driving\" + 0.053*\"sister\" + 0.053*\"my\" + 0.053*\"My\" + 0.053*\"father\" + 0.053*\"of\" + 0.053*\"dance\" + 0.053*\"practice.\" + 0.053*\"around\" + 0.053*\"time\"'), (2, '0.089*\"to\" + 0.051*\"My\" + 0.051*\"my\" + 0.051*\"sister\" + 0.051*\"not\" + 0.051*\"bad\" + 0.051*\"consume.\" + 0.051*\"Sugar\" + 0.051*\"is\" + 0.051*\"father.\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "# Creating the term dictionary of corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " N-Grams as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams('this is a sample text', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Statistical Features\n",
    "![title](./images/tf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.58448290102\n",
      "  (0, 2)\t0.58448290102\n",
      "  (0, 4)\t0.444514311537\n",
      "  (0, 1)\t0.345205016865\n",
      "  (1, 1)\t0.385371627466\n",
      "  (1, 0)\t0.652490884513\n",
      "  (1, 3)\t0.652490884513\n",
      "  (2, 4)\t0.444514311537\n",
      "  (2, 1)\t0.345205016865\n",
      "  (2, 6)\t0.58448290102\n",
      "  (2, 5)\t0.58448290102\n"
     ]
    }
   ],
   "source": [
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Word Embedding (text vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0873396930529\n",
      "[  1.14168017e-03  -4.94242134e-03   2.72238249e-04   3.97438370e-03\n",
      "   1.69398112e-03   2.43922602e-03  -3.20519553e-04  -1.18722732e-03\n",
      "  -4.35135467e-03  -3.56071885e-03   3.99002247e-03  -3.28068179e-03\n",
      "  -1.49453568e-04  -3.82759469e-03  -2.43284900e-04   1.77918072e-03\n",
      "   3.11065342e-05   3.49582359e-03   6.00333533e-06   2.08169455e-03\n",
      "  -4.31511085e-03   2.12243013e-03   7.45210506e-04   3.46274348e-03\n",
      "  -1.47648144e-03  -2.76382617e-03  -2.81338114e-03  -7.59243267e-04\n",
      "   1.94972206e-04   3.54523258e-03  -3.22825788e-03   2.56676367e-03\n",
      "   1.02296274e-03  -4.39176848e-03  -4.27832548e-03   6.48388872e-04\n",
      "   5.47887932e-04  -2.09806324e-03  -4.96958056e-03   2.08194065e-03\n",
      "  -2.02509505e-03  -5.73990343e-04   1.04251027e-03   4.81688976e-03\n",
      "  -2.63691857e-03  -3.34192789e-03   4.24436264e-04  -2.71220435e-03\n",
      "  -3.24598700e-03  -2.61668139e-03   6.59248501e-04   3.95958871e-03\n",
      "  -2.47115782e-03  -4.93136141e-03  -3.11104301e-03  -7.91999686e-04\n",
      "   1.96530949e-03  -1.01992919e-03  -9.50240821e-04  -2.06591314e-04\n",
      "  -3.12367757e-03  -2.95063225e-03  -1.58777717e-03   3.49847996e-03\n",
      "  -2.78011453e-03  -1.72406086e-03   3.00918915e-03   8.79245112e-04\n",
      "   3.99849843e-03   4.24433127e-03   2.80316803e-03  -2.62246421e-03\n",
      "   4.52275295e-03   4.78094257e-03   4.77352366e-03  -4.71356849e-04\n",
      "   1.80890132e-03  -1.92525308e-03  -2.47961935e-03  -3.08465009e-04\n",
      "   1.65019115e-03  -2.86474265e-03  -4.54762997e-03   3.87432403e-03\n",
      "   4.54712426e-03  -2.72506848e-04  -2.30316143e-03   8.46743991e-04\n",
      "  -9.79018514e-04  -4.93637612e-03   1.95224304e-03  -2.90520256e-04\n",
      "   3.51844402e-03  -1.76593685e-03  -3.62128456e-04  -4.57962102e-04\n",
      "  -3.01106041e-03   2.77652498e-03  -3.13955825e-03   2.54331622e-03]\n"
     ]
    }
   ],
   "source": [
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.similarity('data', 'science'))\n",
    "\n",
    "print(model['learning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important tasks of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification\n",
    "![title](./images/classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n",
      "Class_B\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"Their codes are amazing.\"))\n",
    "\n",
    "print(model.classify(\"I don't like their computer.\"))\n",
    "\n",
    "print(model.accuracy(test_corpus)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Class_A       0.50      0.67      0.57         3\n",
      "    Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "avg / total       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "\n",
    "test_data = [] \n",
    "test_labels = [] \n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels) \n",
    "prediction = model.predict(test_vectors)\n",
    "\n",
    "print(classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Matching / Similarity\n",
    "\n",
    "Levenshtein Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.629940788348712\n"
     ]
    }
   ],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an article on analytics vidhya' \n",
    "text2 = 'article on analytics vidhya is about natural language processing'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n",
    "print(cosine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
